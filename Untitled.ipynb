{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6473b5-4472-4d0b-b5b9-603cf33a6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ebd107-a71c-496f-a4d2-ef46ca2b56c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 17:01:57 WARN Utils: Your hostname, TTNPL-kanishkasharma resolves to a loopback address: 127.0.1.1; using 10.1.245.40 instead (on interface wlp0s20f3)\n",
      "25/04/29 17:01:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/29 17:01:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/29 17:01:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31993567-147a-434f-a21f-a10355520c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "                   .option(\"inferSchema\", \"true\") \\\n",
    "                   .csv(\"employee_timeframe_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5d17cb-a55d-43d1-9df6-e2d72f868ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(emp_id=7175690919, designation='A', start_date=1701734385, end_date=1707768029.0, salary=100870),\n",
       " Row(emp_id=7175690919, designation='A', start_date=1707768029, end_date=1709232768.0, salary=189648),\n",
       " Row(emp_id=7175690919, designation='B', start_date=1709232768, end_date=1709250902.0, salary=340522),\n",
       " Row(emp_id=7175690919, designation='E', start_date=1709250902, end_date=1709251128.0, salary=366856),\n",
       " Row(emp_id=7175690919, designation='E', start_date=1709251128, end_date=1709251143.0, salary=376246)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baea0fc7-cc77-43cd-8ee9-dcb6c2b9e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\"start_date\", to_date(from_unixtime(col(\"start_date\")))) \\\n",
    "           .withColumn(\"end_date\", to_date(from_unixtime(col(\"end_date\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fda68de-5ddc-4547-b4a2-ffaabd0d9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+------+\n",
      "|    emp_id|designation|start_date|  end_date|salary|\n",
      "+----------+-----------+----------+----------+------+\n",
      "|7175690919|          A|2023-12-05|2024-02-13|100870|\n",
      "|7175690919|          A|2024-02-13|2024-03-01|189648|\n",
      "|7175690919|          B|2024-03-01|2024-03-01|340522|\n",
      "|7175690919|          E|2024-03-01|2024-03-01|366856|\n",
      "|7175690919|          E|2024-03-01|2024-03-01|376246|\n",
      "|7175690919|          E|2024-03-01|2024-03-01|390315|\n",
      "|7175690919|          E|2024-03-01|2024-03-01|396086|\n",
      "|7175690919|          F|2024-03-01|2024-03-01|399318|\n",
      "|7175690919|          F|2024-03-01|2024-03-01|399589|\n",
      "|7175690919|          F|2024-03-01|      NULL|399589|\n",
      "|3345338467|          F|2022-11-10|2023-10-17|119555|\n",
      "|3345338467|          F|2023-10-17|2024-01-21|248435|\n",
      "|3345338467|          F|2024-01-21|2024-02-29|330464|\n",
      "|3345338467|          F|2024-02-29|2024-02-29|389177|\n",
      "|3345338467|          F|2024-02-29|2024-03-01|395404|\n",
      "|3345338467|          F|2024-03-01|2024-03-01|395883|\n",
      "|3345338467|          F|2024-03-01|2024-03-01|398594|\n",
      "|3345338467|          F|2024-03-01|2024-03-01|398890|\n",
      "|3345338467|          F|2024-03-01|2024-03-01|399382|\n",
      "|3345338467|          F|2024-03-01|      NULL|399382|\n",
      "+----------+-----------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f7b57c-a7fe-406b-a02b-aa3a53ab0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"emp_id\", \"start_date\",\"end_date\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df2 = df1.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "               .filter(col(\"row_num\") == 1) \\\n",
    "               .drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211ccdfe-c138-4d64-a685-d38b176ab0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+----------+------+\n",
      "| emp_id|designation|start_date|  end_date|salary|\n",
      "+-------+-----------+----------+----------+------+\n",
      "| 256817|          E|2022-05-17|2023-04-11|201559|\n",
      "| 256817|          F|2024-02-11|2024-02-14|389512|\n",
      "| 296661|          E|2022-08-30|2023-11-13|286694|\n",
      "| 346216|          E|2024-02-16|2024-02-18|372224|\n",
      "| 561990|          F|2024-01-16|2024-02-21|260903|\n",
      "| 912905|          F|2024-02-29|      NULL|377359|\n",
      "|1426968|          C|2022-04-24|2023-09-01|187474|\n",
      "|1765594|          A|2023-10-02|      NULL|365718|\n",
      "|1875052|          A|2022-05-07|2024-02-11| 66229|\n",
      "|1875052|          D|2024-02-12|2024-02-27|358843|\n",
      "|1875052|          F|2024-03-01|2024-03-01|399627|\n",
      "|1926107|          F|2024-02-16|2024-02-22|386240|\n",
      "|2100181|          D|2023-06-01|2023-11-29|346849|\n",
      "|2100181|          F|2024-02-28|      NULL|399853|\n",
      "|2149103|          E|2022-12-20|2023-10-06|335226|\n",
      "|2245319|          D|2023-10-09|2023-11-28|197371|\n",
      "|2331342|          F|2024-02-28|2024-02-29|399925|\n",
      "|2331342|          F|2024-03-01|2024-03-01|399954|\n",
      "|2830585|          F|2024-02-28|2024-03-01|399073|\n",
      "|2979791|          F|2024-02-18|2024-02-23|399627|\n",
      "+-------+-----------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b198214-2196-47fc-bab3-32284383203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_emp = Window.partitionBy(\"emp_id\").orderBy(\"start_date\")\n",
    "\n",
    "# Get the start_date of the next record\n",
    "df3 = df2.withColumn(\"next_start\", lead(\"start_date\").over(window_emp))\n",
    "\n",
    "# Replace null end_date with next_start - 1 day (for continuity)\n",
    "df4 = df3.withColumn(\n",
    "    \"end_date\",\n",
    "    when(col(\"end_date\").isNull(), date_sub(col(\"next_start\"), 1)).otherwise(col(\"end_date\"))\n",
    ").drop(\"next_start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f52584b7-1c6b-4adf-8615-1690d1f7750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+----------+------+\n",
      "| emp_id|designation|start_date|  end_date|salary|\n",
      "+-------+-----------+----------+----------+------+\n",
      "| 346216|          A|2024-02-02|2024-02-16|345618|\n",
      "| 346216|          E|2024-02-16|2024-02-18|372224|\n",
      "| 346216|          E|2024-02-18|2024-02-29|373525|\n",
      "| 346216|          F|2024-02-29|2024-03-01|397277|\n",
      "| 346216|          F|2024-03-01|2024-02-29|399967|\n",
      "| 346216|          F|2024-03-01|2024-03-01|399967|\n",
      "| 756782|          B|2023-05-22|2023-11-17| 23751|\n",
      "| 756782|          D|2023-11-17|2023-12-14|245353|\n",
      "| 756782|          F|2023-12-14|2023-12-29|270160|\n",
      "| 756782|          F|2023-12-29|      NULL|270160|\n",
      "|1926107|          D|2022-11-15|2023-06-04|161430|\n",
      "|1926107|          E|2023-06-04|2023-12-17|264576|\n",
      "|1926107|          E|2023-12-17|2023-12-19|271112|\n",
      "|1926107|          E|2023-12-19|2024-02-16|350968|\n",
      "|1926107|          F|2024-02-16|2024-02-22|386240|\n",
      "|1926107|          F|2024-02-16|2024-02-16|383211|\n",
      "|1926107|          F|2024-02-22|2024-02-25|390410|\n",
      "|1926107|          F|2024-02-22|2024-02-22|389690|\n",
      "|1926107|          F|2024-02-25|2024-02-26|397278|\n",
      "|1926107|          F|2024-02-26|      NULL|397278|\n",
      "+-------+-----------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c049136-08ae-4f1b-abda-0ec108e57910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"end_date\").isNull(), \"Active\").otherwise(\"Inactive\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51d3082e-0e0f-49ad-b6c5-fb41b51b866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+----------+------+--------+\n",
      "| emp_id|designation|start_date|  end_date|salary|  status|\n",
      "+-------+-----------+----------+----------+------+--------+\n",
      "| 346216|          A|2024-02-02|2024-02-16|345618|Inactive|\n",
      "| 346216|          E|2024-02-16|2024-02-18|372224|Inactive|\n",
      "| 346216|          E|2024-02-18|2024-02-29|373525|Inactive|\n",
      "| 346216|          F|2024-02-29|2024-03-01|397277|Inactive|\n",
      "| 346216|          F|2024-03-01|2024-02-29|399967|Inactive|\n",
      "| 346216|          F|2024-03-01|2024-03-01|399967|Inactive|\n",
      "| 756782|          B|2023-05-22|2023-11-17| 23751|Inactive|\n",
      "| 756782|          D|2023-11-17|2023-12-14|245353|Inactive|\n",
      "| 756782|          F|2023-12-14|2023-12-29|270160|Inactive|\n",
      "| 756782|          F|2023-12-29|      NULL|270160|  Active|\n",
      "|1926107|          D|2022-11-15|2023-06-04|161430|Inactive|\n",
      "|1926107|          E|2023-06-04|2023-12-17|264576|Inactive|\n",
      "|1926107|          E|2023-12-17|2023-12-19|271112|Inactive|\n",
      "|1926107|          E|2023-12-19|2024-02-16|350968|Inactive|\n",
      "|1926107|          F|2024-02-16|2024-02-22|386240|Inactive|\n",
      "|1926107|          F|2024-02-16|2024-02-16|383211|Inactive|\n",
      "|1926107|          F|2024-02-22|2024-02-25|390410|Inactive|\n",
      "|1926107|          F|2024-02-22|2024-02-22|389690|Inactive|\n",
      "|1926107|          F|2024-02-25|2024-02-26|397278|Inactive|\n",
      "|1926107|          F|2024-02-26|      NULL|397278|  Active|\n",
      "+-------+-----------+----------+----------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7f9c1-f43d-410a-beb3-83c0d8cf852f",
   "metadata": {},
   "source": [
    "Append-Only Yearly Tables:\n",
    "employee_leave_quota_data.csv ‚Üí Yearly Quota Table\n",
    "\n",
    "employee_leave_calendar_data.csv ‚Üí Yearly Holiday Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ca7bb0-7c60-4f11-af25-f4d7c9f599fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar = spark.read.option(\"header\", True).csv(\"employee_leave_calendar_data.csv\") \n",
    "\n",
    "calendar_data = df_calendar.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "#holiday_df.write.format(\"delta\").mode(\"append\").partitionBy(\"date\") \\\n",
    "#       .save(\"s3://your-bucket/output/leave_calendar/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b96909b-e851-4e1a-9a9b-5c1857a47649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_leave_quota = spark.read.option(\"header\", \"true\") \\\n",
    "                   .option(\"inferSchema\", \"true\") \\\n",
    "                   .csv(\"employee_leave_quota_data.csv\")\n",
    "\n",
    "quota_data = df_leave_quota.withColumn(\"leave_quota\", df_leave_quota[\"leave_quota\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"year\", df_leave_quota[\"year\"].cast(\"int\"))\n",
    "\n",
    "#quota_df = spark.read.option(\"header\", \"true\").csv(\"s3://your-bucket/employee_leave_quota_data.csv\")\n",
    "\n",
    "# Ensure data types\n",
    "#quota_df = quota_df.withColumn(\"leave_quota\", quota_df[\"leave_quota\"].cast(\"int\")) \\\n",
    "                   #.withColumn(\"year\", quota_df[\"year\"].cast(\"int\"))\n",
    "\n",
    "# Write as append-only\n",
    "#quota_df.write.format(\"delta\").mode(\"append\").partitionBy(\"year\") \\\n",
    "        #.save(\"s3://your-bucket/output/leave_quota/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76066880-e9f9-4bc8-ba55-8fd93498711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "leave_data = spark.read.option(\"header\", \"true\") \\\n",
    "                   .option(\"inferSchema\", \"true\") \\\n",
    "                   .csv(\"employee_leave_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277aa3e4-4fde-4277-8891-f854bfb51667",
   "metadata": {},
   "source": [
    "For each (emp_id, date):\n",
    "\n",
    "Count how many times each status occurs: \"ACTIVE\" and \"CANCELLED\"\n",
    "\n",
    "Apply logic:\n",
    "\n",
    "If \"CANCELLED\" count > \"ACTIVE\" count ‚Üí status is \"CANCELLED\"\n",
    "\n",
    "If \"ACTIVE\" count > \"CANCELLED\" count ‚Üí status is \"ACTIVE\"\n",
    "\n",
    "If counts are equal ‚Üí status is \"CANCELLED\" (since cancel overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e67469e-0340-4a1c-873a-ce20e41f405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure date is in correct format\n",
    "leave_data = leave_data.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    "\n",
    "# Step 2: Map status to two count columns\n",
    "leave_status_counts = leave_data.withColumn(\"is_active\", when(col(\"status\") == \"ACTIVE\", 1).otherwise(0)) \\\n",
    "                                .withColumn(\"is_cancelled\", when(col(\"status\") == \"CANCELLED\", 1).otherwise(0))\n",
    "\n",
    "# Step 3: Group by emp_id + date and sum status flags\n",
    "grouped = leave_status_counts.groupBy(\"emp_id\", \"date\") \\\n",
    "    .agg(\n",
    "        sum(\"is_active\").alias(\"active_count\"),\n",
    "        sum(\"is_cancelled\").alias(\"cancelled_count\")\n",
    "    )\n",
    "\n",
    "# Step 4: Apply status logic based on counts\n",
    "leave_data_updated = grouped.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"cancelled_count\") >= col(\"active_count\"), \"CANCELLED\").otherwise(\"ACTIVE\")\n",
    ").select(\"emp_id\", \"date\", \"status\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac3ff38-83e8-449c-aba4-042dfca8f71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 17:03:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "dupes = leave_data_updated.groupBy(\"emp_id\", \"date\").count().filter(\"count > 1\")\n",
    "if dupes.count() > 0:\n",
    "    print(\"‚ö†Ô∏è Duplicate ACTIVE leave records found:\")\n",
    "    dupes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0dd726a-9724-47f4-98cc-56ea5bc9b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/29 17:03:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/29 17:03:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 30:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|    emp_id|      date|status|\n",
      "+----------+----------+------+\n",
      "|7175690919|2023-05-19|ACTIVE|\n",
      "| 272449155|2023-11-05|ACTIVE|\n",
      "|9154542239|2023-06-08|ACTIVE|\n",
      "|9154542239|2023-07-10|ACTIVE|\n",
      "|1260589765|2023-05-16|ACTIVE|\n",
      "|6727837063|2023-02-12|ACTIVE|\n",
      "|9623856554|2024-02-11|ACTIVE|\n",
      "|3368105384|2023-05-10|ACTIVE|\n",
      "|6619102777|2023-06-18|ACTIVE|\n",
      "|6619102777|2023-07-28|ACTIVE|\n",
      "|5568316074|2023-02-03|ACTIVE|\n",
      "| 706688810|2024-07-20|ACTIVE|\n",
      "|4530872653|2024-03-13|ACTIVE|\n",
      "|5033623298|2023-09-21|ACTIVE|\n",
      "|8022137890|2023-04-13|ACTIVE|\n",
      "|5468748102|2024-09-11|ACTIVE|\n",
      "|8328042768|2023-07-10|ACTIVE|\n",
      "|8523798777|2024-04-23|ACTIVE|\n",
      "|3286077338|2024-01-24|ACTIVE|\n",
      "| 958054928|2024-10-15|ACTIVE|\n",
      "+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "leave_data_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca827c4-c968-4918-bc7f-017a97cae841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_status.write.mode(\"overwrite\").format(\"delta\") \\\n",
    "    #.save(\"s3://your-bucket/processed/employee_leave_daily/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9233f-fc93-4604-9c83-ac7f18e2be28",
   "metadata": {},
   "source": [
    "\n",
    "Generate a daily table (at 7:00 UTC) showing currently active employees by designation and count.\n",
    "\n",
    "Active Employees by Designation (Daily @ 7:00 UTC)\n",
    "Input:\n",
    "\n",
    "employee_timeframe_data (already processed and includes status)\n",
    "\n",
    "Logic:\n",
    "\n",
    "Filter records where status = 'Active'\n",
    "\n",
    "Group by designation\n",
    "\n",
    "Count employees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de86edc-773b-47c1-b298-1c90a920d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#employee_timeline = spark.read.format(\"delta\").load(\"s3://your-bucket/employee_timeframe_delta/\")\n",
    "\n",
    "active_employees = df4.filter(col(\"status\") == \"Active\") \\\n",
    "    .groupBy(\"designation\") \\\n",
    "    .agg(count(\"emp_id\").alias(\"active_emp_count\"))\\\n",
    "    .sort(\"active_emp_count\")\n",
    "\n",
    "#active_employees.write.mode(\"overwrite\").format(\"delta\").save(\"s3://your-bucket/reports/active_employee_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5d4b8fd-48bb-4a90-a7f2-448ef9168741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|designation|active_emp_count|\n",
      "+-----------+----------------+\n",
      "|          A|            2038|\n",
      "|          B|            2471|\n",
      "|          C|            3184|\n",
      "|          D|            4771|\n",
      "|          E|            9188|\n",
      "|          F|           58006|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "active_employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4da78-ca25-4628-b3a6-991bf6e82b3e",
   "metadata": {},
   "source": [
    "Potential Leave Abuse > 8% of Working Days (Daily @ 7:00 UTC)\n",
    "Inputs:\n",
    "\n",
    "employee_leave_data.csv\n",
    "\n",
    "employee_leave_calendar_data.csv\n",
    "\n",
    "Generate working days from tomorrow to end of year\n",
    "\n",
    "Exclude:\n",
    "\n",
    "Weekends\n",
    "\n",
    "Public holidays\n",
    "\n",
    "Cancelled leaves\n",
    "\n",
    "Logic:\n",
    "\n",
    "Calculate remaining working days this year.\n",
    "\n",
    "Filter employee_leave_data:\n",
    "\n",
    "status = ACTIVE\n",
    "\n",
    "leave date in future\n",
    "\n",
    "not weekend, not holiday\n",
    "\n",
    "Count leaves per employee\n",
    "\n",
    "If leave_count > 8% of working days, flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4fc2f92-d5e6-4ebd-9d3c-179358457c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import max\n",
    "\n",
    "# ========== Step 0: Set target year ==========\n",
    "target_year = 2023  # change to 2024, 2025 etc. as needed\n",
    "\n",
    "today = datetime.utcnow().date()\n",
    "start_date = max(today, datetime(target_year, 1, 1).date())  # don't go back in time\n",
    "end_date = datetime(target_year, 12, 31).date()\n",
    "\n",
    "current_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_of_year_str = end_date.strftime('%Y-%m-%d')\n",
    "run_date_str = today.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2201e377-ca85-48a3-8dfc-9e98304f1188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Running for date range: 2024-04-25 to 2024-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü° Active leave records: 1404293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÜ Total working days left: 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Leaves on working days: 978178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-------+----------+\n",
      "|    emp_id|leaves_taken|     leave_percent|flagged|  run_date|\n",
      "+----------+------------+------------------+-------+----------+\n",
      "|2624934405|           9| 5.113636363636364|     No|2024-04-25|\n",
      "|6865520457|           9| 5.113636363636364|     No|2024-04-25|\n",
      "|4070419776|          12|6.8181818181818175|     No|2024-04-25|\n",
      "|1991864484|          13| 7.386363636363637|     No|2024-04-25|\n",
      "| 993324297|          14| 7.954545454545454|     No|2024-04-25|\n",
      "|2857000361|          15| 8.522727272727272|    Yes|2024-04-25|\n",
      "|5168383199|          11|              6.25|     No|2024-04-25|\n",
      "|4197767131|          12|6.8181818181818175|     No|2024-04-25|\n",
      "|7194063534|          10| 5.681818181818182|     No|2024-04-25|\n",
      "|2343017910|           9| 5.113636363636364|     No|2024-04-25|\n",
      "|1821916857|           7| 3.977272727272727|     No|2024-04-25|\n",
      "|7359053082|          10| 5.681818181818182|     No|2024-04-25|\n",
      "| 678878907|          11|              6.25|     No|2024-04-25|\n",
      "|9748343987|           7| 3.977272727272727|     No|2024-04-25|\n",
      "|1137144613|           9| 5.113636363636364|     No|2024-04-25|\n",
      "|4782641341|          12|6.8181818181818175|     No|2024-04-25|\n",
      "|2705940958|           6|3.4090909090909087|     No|2024-04-25|\n",
      "|3841166341|          11|              6.25|     No|2024-04-25|\n",
      "|5704161349|           3|1.7045454545454544|     No|2024-04-25|\n",
      "|9084342788|          11|              6.25|     No|2024-04-25|\n",
      "+----------+------------+------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, to_date, dayofweek, explode, sequence, countDistinct, expr, year\n",
    ")\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# ========== Step 1: Setup Dates ==========\n",
    "# today = datetime.utcnow().date()\n",
    "today = datetime(2024,4,25) # when i explicitly gave the date\n",
    "current_date_str = today.strftime('%Y-%m-%d')\n",
    "end_of_year_str = f\"{today.year}-12-31\"\n",
    "run_date_str = today.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "print(\"üìÖ Running for date range:\", current_date_str, \"to\", end_of_year_str)\n",
    "\n",
    "# ========== Step 2: Prepare Leave Data ==========\n",
    "leave_data_final = leave_data_updated.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "active_leaves = leave_data_final.filter(\n",
    "    (col(\"status\") == \"ACTIVE\") &\n",
    "    (col(\"date\") >= lit(current_date_str))\n",
    ").dropDuplicates([\"emp_id\", \"date\"])\n",
    "\n",
    "print(\"üü° Active leave records:\", active_leaves.count())\n",
    "\n",
    "# ========== Step 3: Prepare Holiday Data ==========\n",
    "holidays = calendar_data \\\n",
    "    .withColumn(\"date\", to_date(\"date\")) \\\n",
    "    .filter(col(\"date\") >= lit(current_date_str)) \\\n",
    "    .select(\"date\").distinct() \\\n",
    "    .withColumn(\"holiday\", lit(1))\n",
    "\n",
    "# ========== Step 4: Generate Working Days ==========\n",
    "date_range = spark.createDataFrame([()]).select(\n",
    "    explode(sequence(to_date(lit(current_date_str)), to_date(lit(end_of_year_str)))).alias(\"date\")\n",
    ")\n",
    "\n",
    "working_days = date_range \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"date\")) \\\n",
    "    .filter(~col(\"day_of_week\").isin([1, 7])) \\\n",
    "    .drop(\"day_of_week\") \\\n",
    "    .join(holidays, on=\"date\", how=\"left_anti\") \\\n",
    "    .persist()\n",
    "\n",
    "total_working_days = working_days.count()\n",
    "print(\"üìÜ Total working days left:\", total_working_days)\n",
    "\n",
    "if total_working_days == 0:\n",
    "    print(\"‚ö†Ô∏è No valid working days found. Exiting early.\")\n",
    "else:\n",
    "    # ========== Step 5: Join Leaves with Working Days ==========\n",
    "    leaves_on_working_days = active_leaves.join(working_days, on=\"date\", how=\"inner\") \\\n",
    "        .select(\"emp_id\", \"date\").distinct()\n",
    "\n",
    "    print(\"üìå Leaves on working days:\", leaves_on_working_days.count())\n",
    "\n",
    "    # ========== Step 6: Count Leaves and Flag ==========\n",
    "    emp_leave_counts = leaves_on_working_days.groupBy(\"emp_id\") \\\n",
    "        .agg(countDistinct(\"date\").alias(\"leaves_taken\"))\n",
    "\n",
    "    flagged = emp_leave_counts.withColumn(\n",
    "        \"leave_percent\",\n",
    "        (col(\"leaves_taken\") / lit(total_working_days)) * 100\n",
    "    ).withColumn(\n",
    "        \"flagged\", expr(\"CASE WHEN leave_percent > 8 THEN 'Yes' ELSE 'No' END\")\n",
    "    ).withColumn(\"run_date\", lit(run_date_str))\n",
    "\n",
    "    # Show result\n",
    "    flagged.show()\n",
    "\n",
    "    # ========== Step 7: Save to S3 (optional) ==========\n",
    "    # flagged.write.mode(\"overwrite\") \\\n",
    "    #     .format(\"delta\") \\\n",
    "    #     .partitionBy(\"run_date\") \\\n",
    "    #     .save(\"s3://your-bucket/reports/daily_leave_abuse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58dcd169-4015-4d41-ae11-92adbb6034df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:>                                                         (0 + 6) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Employees flagged (leave > 8%): 6515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "count_flagged = flagged.filter(col(\"flagged\") == \"Yes\").count()\n",
    "print(f\"‚úÖ Employees flagged (leave > 8%): {count_flagged}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f0d5d53-00e0-4e78-98f2-d25b97987cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------------+-------+----------+\n",
      "|   emp_id|leaves_taken|     leave_percent|flagged|  run_date|\n",
      "+---------+------------+------------------+-------+----------+\n",
      "|154225493|           6|3.4090909090909087|     No|2024-04-25|\n",
      "+---------+------------+------------------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_flagged = flagged.filter(col(\"emp_id\") == '154225493')\n",
    "employee_flagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cefebdd1-6b68-4bb8-ad18-d227faf1122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Days Count: 176\n",
      "DataFrame[date: date, holiday: int]\n"
     ]
    }
   ],
   "source": [
    "print(\"Working Days Count:\", working_days.count())\n",
    "print(holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeb848-6cd2-41b5-868f-a3e5c14fd59a",
   "metadata": {},
   "source": [
    "Leave Quota Overuse >80% (Monthly on 1st @ 7:00 UTC)\n",
    "Inputs:\n",
    "\n",
    "employee_leave_data.csv\n",
    "\n",
    "employee_leave_quota_data.csv\n",
    "\n",
    "Logic:\n",
    "\n",
    "For current year, count leaves where status = ACTIVE\n",
    "\n",
    "Join with leave quota data\n",
    "\n",
    "If (leave_taken / quota) > 0.8, flag\n",
    "\n",
    "Group by manager (placeholder or you can skip the file-per-manager if no manager field is present)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "634f6bfd-abe8-4b9b-87d4-f32dbe75ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, to_date, year, countDistinct, lit, expr\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# # ========== Step 1: Setup ==========\n",
    "# #today = datetime.utcnow().date()\n",
    "# today = datetime(2024,1,1) # when i explicitly gave the date\n",
    "# current_year = today.year\n",
    "# run_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "# # base_path = f\"./monthly_leave_reports/run_date={run_date_str}\"\n",
    "\n",
    "# # Ensure output dir exists\n",
    "# # os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# # ========== Step 2: Prepare Clean Data ==========\n",
    "# leave_data_updated = leave_data_updated.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "# # Filter ACTIVE leaves for current year (no deduping needed)\n",
    "# valid_leaves = leave_data_updated.filter(\n",
    "#     (col(\"status\") == \"ACTIVE\") &\n",
    "#     (year(col(\"date\")) == current_year)\n",
    "# )\n",
    "\n",
    "# # Count unique leave days per employee\n",
    "# leaves_taken = valid_leaves.groupBy(\"emp_id\") \\\n",
    "#     .agg(countDistinct(\"date\").alias(\"leaves_taken\"))\n",
    "\n",
    "# # Join with leave quota and calculate leave percentage\n",
    "# leave_usage = leaves_taken.join(quota_data, on=\"emp_id\", how=\"inner\") \\\n",
    "#     .filter(col(\"year\") == lit(current_year)) \\\n",
    "#     .withColumn(\"leave_percent\", (col(\"leaves_taken\") / col(\"leave_quota\")) * 100) \\\n",
    "#     .withColumn(\"flagged\", expr(\"CASE WHEN leave_percent > 80 THEN 'Yes' ELSE 'No' END\")) \\\n",
    "#     .filter(col(\"flagged\") == \"Yes\") \\\n",
    "#     .withColumn(\"run_date\", lit(run_date_str))\n",
    "\n",
    "# leave_usage.show()\n",
    "\n",
    "# # ========== Step 3: Write Report Per Employee (no duplicates) ==========\n",
    "# # flagged_emps = leave_usage.select(\"emp_id\").rdd.map(lambda r: r[\"emp_id\"]).collect()\n",
    "\n",
    "# # for emp_id in flagged_emps:\n",
    "# #     emp_dir = os.path.join(base_path, f\"emp_id={emp_id}\")\n",
    "# #     os.makedirs(emp_dir, exist_ok=True)\n",
    "    \n",
    "# #     file_path = os.path.join(emp_dir, \"report.txt\")\n",
    "    \n",
    "# #     # Skip if already exists (idempotency)\n",
    "# #     if os.path.exists(file_path):\n",
    "# #         print(f\"üü° Report already exists for emp_id={emp_id}. Skipping.\")\n",
    "# #         continue\n",
    "\n",
    "# #     # Filter employee-specific data and write as text\n",
    "# #     emp_df = leave_usage.filter(col(\"emp_id\") == emp_id)\n",
    "# #     emp_rows = emp_df.collect()\n",
    "\n",
    "# #     with open(file_path, \"w\") as f:\n",
    "# #         for row in emp_rows:\n",
    "# #             f.write(str(row.asDict()) + \"\\n\")\n",
    "\n",
    "# #     print(f\"‚úÖ Written report for emp_id={emp_id}\")\n",
    "\n",
    "# # print(\"üéâ All reports generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5110cc1a-9ce2-49df-a5fc-52fc916eeab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Reporting for period: Jan 1 to 2024-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+----+-----------------+-------+----------+\n",
      "|    emp_id|leaves_taken|leave_quota|year|    leave_percent|flagged|  run_date|\n",
      "+----------+------------+-----------+----+-----------------+-------+----------+\n",
      "|2451985258|          23|         24|2024|95.83333333333334|    Yes|2024-11-01|\n",
      "|7099944869|          18|         22|2024|81.81818181818183|    Yes|2024-11-01|\n",
      "|4399264584|          21|         24|2024|             87.5|    Yes|2024-11-01|\n",
      "|2238865106|          18|         21|2024|85.71428571428571|    Yes|2024-11-01|\n",
      "|3186300822|          17|         20|2024|             85.0|    Yes|2024-11-01|\n",
      "| 715022541|          18|         20|2024|             90.0|    Yes|2024-11-01|\n",
      "|3799324173|          17|         21|2024|80.95238095238095|    Yes|2024-11-01|\n",
      "|9320403989|          20|         22|2024| 90.9090909090909|    Yes|2024-11-01|\n",
      "|7430004905|          19|         22|2024|86.36363636363636|    Yes|2024-11-01|\n",
      "|1486627765|          19|         22|2024|86.36363636363636|    Yes|2024-11-01|\n",
      "|6989691021|          21|         25|2024|             84.0|    Yes|2024-11-01|\n",
      "| 476742904|          18|         21|2024|85.71428571428571|    Yes|2024-11-01|\n",
      "| 914558415|          18|         21|2024|85.71428571428571|    Yes|2024-11-01|\n",
      "|8107219781|          18|         20|2024|             90.0|    Yes|2024-11-01|\n",
      "|1936279795|          17|         20|2024|             85.0|    Yes|2024-11-01|\n",
      "|1189495363|          19|         23|2024| 82.6086956521739|    Yes|2024-11-01|\n",
      "|7786726416|          17|         21|2024|80.95238095238095|    Yes|2024-11-01|\n",
      "|3891731279|          17|         21|2024|80.95238095238095|    Yes|2024-11-01|\n",
      "|4244168607|          20|         24|2024|83.33333333333334|    Yes|2024-11-01|\n",
      "|7903764098|          20|         21|2024|95.23809523809523|    Yes|2024-11-01|\n",
      "+----------+------------+-----------+----+-----------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, countDistinct, lit, expr\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "\n",
    "# ========== Step 1: Setup Reporting Dates ==========\n",
    "# Simulate: Run on the 1st of any month\n",
    "#today = datetime.utcnow().date()\n",
    "today = datetime(2024, 11, 1).date()  # ‚Üê change to the 1st of any month you're testing\n",
    "\n",
    "# Calculate end of previous month as cutoff\n",
    "report_cutoff = today - relativedelta(days=1)  # e.g., 2024-04-30\n",
    "report_cutoff_str = report_cutoff.strftime(\"%Y-%m-%d\")\n",
    "run_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "current_year = today.year\n",
    "\n",
    "print(f\"üìä Reporting for period: Jan 1 to {report_cutoff_str}\")\n",
    "\n",
    "# ========== Step 2: Prepare Clean Leave Data ==========\n",
    "leave_data_updated = leave_data_updated.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "# Filter ACTIVE leaves in the current year up to the cutoff\n",
    "valid_leaves = leave_data_updated.filter(\n",
    "    (col(\"status\") == \"ACTIVE\") &\n",
    "    (year(col(\"date\")) == current_year) &\n",
    "    (col(\"date\") <= lit(report_cutoff_str))\n",
    ")\n",
    "\n",
    "# Generate date range Jan 1 ‚Üí report_cutoff\n",
    "date_range = spark.createDataFrame([()]).select(\n",
    "    explode(sequence(\n",
    "        to_date(lit(f\"{current_year}-01-01\")),\n",
    "        to_date(lit(report_cutoff_str))\n",
    "    )).alias(\"date\")\n",
    ")\n",
    "\n",
    "# Remove weekends\n",
    "working_days = date_range.withColumn(\"dow\", dayofweek(\"date\")) \\\n",
    "    .filter(~col(\"dow\").isin([1, 7])) \\\n",
    "    .drop(\"dow\")\n",
    "\n",
    "# Join holiday calendar and remove holiday dates\n",
    "holidays = calendar_data.withColumn(\"date\", to_date(\"date\")).select(\"date\").distinct()\n",
    "working_days = working_days.join(holidays, on=\"date\", how=\"left_anti\")\n",
    "\n",
    "# Join with valid leave data\n",
    "leaves_on_working_days = valid_leaves.join(working_days, on=\"date\", how=\"inner\")\n",
    "\n",
    "# Count real working leaves\n",
    "leaves_taken = leaves_on_working_days.groupBy(\"emp_id\") \\\n",
    "    .agg(countDistinct(\"date\").alias(\"leaves_taken\"))\n",
    "\n",
    "# ========== Step 3: Join with Quota & Flag ==========\n",
    "leave_usage = leaves_taken.join(quota_data, on=\"emp_id\", how=\"inner\") \\\n",
    "    .filter(col(\"year\") == lit(current_year)) \\\n",
    "    .withColumn(\"leave_percent\", (col(\"leaves_taken\") / col(\"leave_quota\")) * 100) \\\n",
    "    .withColumn(\"flagged\", expr(\"CASE WHEN leave_percent > 80 THEN 'Yes' ELSE 'No' END\")) \\\n",
    "    .filter(col(\"flagged\") == \"Yes\") \\\n",
    "    .withColumn(\"run_date\", lit(run_date_str))\n",
    "\n",
    "leave_usage.show()\n",
    "\n",
    "# ========== Step 4: Write Report Per Employee (no duplicates) ==========\n",
    "# base_path = f\"./monthly_leave_reports/run_date={run_date_str}\"\n",
    "# os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# flagged_emps = leave_usage.select(\"emp_id\").rdd.map(lambda r: r[\"emp_id\"]).collect()\n",
    "\n",
    "# for emp_id in flagged_emps:\n",
    "#     emp_dir = os.path.join(base_path, f\"emp_id={emp_id}\")\n",
    "#     os.makedirs(emp_dir, exist_ok=True)\n",
    "\n",
    "#     file_path = os.path.join(emp_dir, \"report.txt\")\n",
    "\n",
    "#     # Skip if already exists (idempotency)\n",
    "#     if os.path.exists(file_path):\n",
    "#         print(f\"üü° Report already exists for emp_id={emp_id}. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     # Write this employee‚Äôs report\n",
    "#     emp_df = leave_usage.filter(col(\"emp_id\") == emp_id)\n",
    "#     rows = emp_df.collect()\n",
    "\n",
    "#     with open(file_path, \"w\") as f:\n",
    "#         for row in rows:\n",
    "#             f.write(str(row.asDict()) + \"\\n\")\n",
    "\n",
    "#     print(f\"‚úÖ Report written for emp_id={emp_id}\")\n",
    "\n",
    "# print(\"üéâ Monthly leave quota reports completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "394358b6-95cd-4dd1-92bd-00ee95b38fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:=================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|    emp_id|leaves_taken|\n",
      "+----------+------------+\n",
      "|1226091381|          14|\n",
      "+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_leaves = leaves_taken.filter(col(\"emp_id\") == '1226091381')\n",
    "employee_leaves.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e84cdfe8-f80f-4176-807e-2f0f82346ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 153:==============>                                          (2 + 6) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+----+-----------------+-------+----------+\n",
      "|    emp_id|leaves_taken|leave_quota|year|    leave_percent|flagged|  run_date|\n",
      "+----------+------------+-----------+----+-----------------+-------+----------+\n",
      "|2451985258|          23|         24|2024|95.83333333333334|    Yes|2024-11-01|\n",
      "|7099944869|          18|         22|2024|81.81818181818183|    Yes|2024-11-01|\n",
      "|4399264584|          21|         24|2024|             87.5|    Yes|2024-11-01|\n",
      "|2238865106|          18|         21|2024|85.71428571428571|    Yes|2024-11-01|\n",
      "|3186300822|          17|         20|2024|             85.0|    Yes|2024-11-01|\n",
      "| 715022541|          18|         20|2024|             90.0|    Yes|2024-11-01|\n",
      "|3799324173|          17|         21|2024|80.95238095238095|    Yes|2024-11-01|\n",
      "|9320403989|          20|         22|2024| 90.9090909090909|    Yes|2024-11-01|\n",
      "|7430004905|          19|         22|2024|86.36363636363636|    Yes|2024-11-01|\n",
      "|1486627765|          19|         22|2024|86.36363636363636|    Yes|2024-11-01|\n",
      "|6989691021|          21|         25|2024|             84.0|    Yes|2024-11-01|\n",
      "| 476742904|          18|         21|2024|85.71428571428571|    Yes|2024-11-01|\n",
      "| 914558415|          18|         21|2024|85.71428571428571|    Yes|2024-11-01|\n",
      "|8107219781|          18|         20|2024|             90.0|    Yes|2024-11-01|\n",
      "|1936279795|          17|         20|2024|             85.0|    Yes|2024-11-01|\n",
      "|1189495363|          19|         23|2024| 82.6086956521739|    Yes|2024-11-01|\n",
      "|7786726416|          17|         21|2024|80.95238095238095|    Yes|2024-11-01|\n",
      "|3891731279|          17|         21|2024|80.95238095238095|    Yes|2024-11-01|\n",
      "|4244168607|          20|         24|2024|83.33333333333334|    Yes|2024-11-01|\n",
      "|7903764098|          20|         21|2024|95.23809523809523|    Yes|2024-11-01|\n",
      "+----------+------------+-----------+----+-----------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "leave_usage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1708068a-1472-4bf7-bb85-c830da9ebcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:=================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total flagged employees: 2439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "flagged_count = leave_usage.count()\n",
    "print(f\"üîç Total flagged employees: {flagged_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43da287c-a135-4869-bf3a-75d61bc94dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### from pyspark.sql.functions import col, to_date, year, countDistinct, lit, expr\n",
    "# from datetime import datetime\n",
    "# import boto3\n",
    "\n",
    "# # ========== Step 1: Setup ==========\n",
    "# today = datetime.utcnow().date()\n",
    "# current_year = today.year\n",
    "# run_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# bucket = \"your-bucket\"  # ‚Üê replace with your S3 bucket name\n",
    "# s3_prefix_base = f\"monthly_leave_reports/run_date={run_date_str}\"\n",
    "\n",
    "# s3 = boto3.client(\"s3\")\n",
    "\n",
    "# def file_exists_in_s3(bucket, key_prefix):\n",
    "#     \"\"\"Check if a file already exists in S3 for the given prefix (per employee).\"\"\"\n",
    "#     response = s3.list_objects_v2(Bucket=bucket, Prefix=key_prefix)\n",
    "#     return \"Contents\" in response\n",
    "\n",
    "# # ========== Step 2: Prepare Clean Data ==========\n",
    "# # Ensure 'date' column is in proper DateType\n",
    "# employee_leaves_df = employee_leaves_df.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "# # Filter ACTIVE leaves within the current year (no deduping needed)\n",
    "# valid_leaves = employee_leaves_df.filter(\n",
    "#     (col(\"status\") == \"ACTIVE\") &\n",
    "#     (year(col(\"date\")) == current_year)\n",
    "# )\n",
    "\n",
    "# # Count leaves taken per employee\n",
    "# leaves_taken = valid_leaves.groupBy(\"emp_id\") \\\n",
    "#     .agg(countDistinct(\"date\").alias(\"leaves_taken\"))\n",
    "\n",
    "# # Join with leave quota and compute leave percent\n",
    "# leave_usage = leaves_taken.join(employee_leaves_quota_df, on=\"emp_id\", how=\"inner\") \\\n",
    "#     .filter(col(\"year\") == lit(current_year)) \\\n",
    "#     .withColumn(\"leave_percent\", (col(\"leaves_taken\") / col(\"leave_quota\")) * 100) \\\n",
    "#     .withColumn(\"flagged\", expr(\"CASE WHEN leave_percent > 80 THEN 'Yes' ELSE 'No' END\")) \\\n",
    "#     .filter(col(\"flagged\") == \"Yes\") \\\n",
    "#     .withColumn(\"run_date\", lit(run_date_str))\n",
    "\n",
    "# # ========== Step 3: Write One Report per Employee (idempotent) ==========\n",
    "# flagged_employees = leave_usage.select(\"emp_id\").rdd.map(lambda row: row[\"emp_id\"]).collect()\n",
    "\n",
    "# for emp_id in flagged_employees:\n",
    "#     s3_key_prefix = f\"{s3_prefix_base}/emp_id={emp_id}/\"\n",
    "    \n",
    "#     if file_exists_in_s3(bucket, s3_key_prefix):\n",
    "#         print(f\"üü° Skipping emp_id={emp_id}, report already exists.\")\n",
    "#         continue\n",
    "\n",
    "#     emp_df = leave_usage.filter(col(\"emp_id\") == emp_id)\n",
    "\n",
    "#     # Save as text file (1 file per emp_id)\n",
    "#     emp_df.coalesce(1).write.mode(\"overwrite\") \\\n",
    "#         .format(\"text\") \\\n",
    "#         .option(\"header\", True) \\\n",
    "#         .save(f\"s3://{bucket}/{s3_key_prefix}\")\n",
    "\n",
    "#     print(f\"‚úÖ Report written for emp_id={emp_id}\")\n",
    "\n",
    "# print(\"üéâ Monthly leave quota reports completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78b015-c7a0-49f8-8764-be970c604e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba343986-aed7-49eb-9096-4bb8a3de0c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
