{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c1a4e3-3ca4-402b-be0a-9a0440c7b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04180557-f65c-4f9d-99e3-492fdadc493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 05:41:21 WARN Utils: Your hostname, TTNPL-kanishkasharma resolves to a loopback address: 127.0.1.1; using 192.168.1.54 instead (on interface wlp0s20f3)\n",
      "25/05/08 05:41:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/08 05:41:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Employee_Reporting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c029e-77d0-413a-94ed-8e082b9ca771",
   "metadata": {},
   "source": [
    "Append-Only Yearly Tables:\n",
    "employee_leave_quota_data.csv → Yearly Quota Table\n",
    "\n",
    "employee_leave_calendar_data.csv → Yearly Holiday Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240412c3-550f-45c1-ac42-faac8cc8355e",
   "metadata": {},
   "source": [
    "KEEP ACTIVE EMPLOYEES ONLY FOR LEAVE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb33bcbd-5ad1-4669-8dec-fce060ddfde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|reason|      date|\n",
      "+------+----------+\n",
      "|1KDIHR|2023-07-28|\n",
      "|L52O0S|2023-08-10|\n",
      "|CFBUWO|2023-08-04|\n",
      "|EZNHM2|2023-06-22|\n",
      "|HTI2Z9|2023-07-24|\n",
      "|LUISBT|2023-03-04|\n",
      "|E8AF7Z|2023-12-09|\n",
      "|FJ2A08|2023-06-03|\n",
      "|4EIDAB|2023-07-16|\n",
      "|9BJCCY|2023-08-03|\n",
      "|KM7BZW|2024-04-19|\n",
      "|OS7EJN|2024-05-11|\n",
      "|LBRNTP|2024-12-15|\n",
      "|L095Q3|2024-07-19|\n",
      "|180M0W|2024-04-26|\n",
      "|NP47QH|2024-02-28|\n",
      "|QZDYCX|2024-03-11|\n",
      "|606QBS|2024-02-21|\n",
      "|H3AJUH|2024-08-08|\n",
      "|JK1REQ|2024-01-18|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_calendar = spark.read.option(\"header\", True).csv(\"employee_leave_calendar_data.csv\") \n",
    "\n",
    "calendar_data = df_calendar.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "calendar_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7df0187-fd9e-46b3-942e-8ac7aa9f06e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----+\n",
      "|    emp_id|leave_quota|year|\n",
      "+----------+-----------+----+\n",
      "|7175690919|         23|2023|\n",
      "|7175690919|         26|2024|\n",
      "|3345338467|         24|2023|\n",
      "|3345338467|         28|2024|\n",
      "|4047753983|         26|2023|\n",
      "|4047753983|         24|2024|\n",
      "|5551916669|         22|2023|\n",
      "|5551916669|         21|2024|\n",
      "|6533954001|         28|2023|\n",
      "|6533954001|         30|2024|\n",
      "|7170426383|         27|2023|\n",
      "|7170426383|         23|2024|\n",
      "|3755067549|         27|2023|\n",
      "|3755067549|         20|2024|\n",
      "| 272449155|         20|2023|\n",
      "| 272449155|         28|2024|\n",
      "|2903567372|         28|2023|\n",
      "|2903567372|         28|2024|\n",
      "|4397504229|         20|2023|\n",
      "|4397504229|         28|2024|\n",
      "+----------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_leave_quota = spark.read.option(\"header\", \"true\") \\\n",
    "                   .option(\"inferSchema\", \"true\") \\\n",
    "                   .csv(\"employee_leave_quota_data.csv\")\n",
    "\n",
    "quota_data = df_leave_quota.withColumn(\"leave_quota\", df_leave_quota[\"leave_quota\"].cast(\"int\")) \\\n",
    "                   .withColumn(\"year\", df_leave_quota[\"year\"].cast(\"int\"))\n",
    "quota_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d291d4a-d3a0-4723-a2fe-e0d3eb92e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    emp_id|\n",
      "+----------+\n",
      "|7601580536|\n",
      "|5897722297|\n",
      "|5466669121|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "quota_data.groupBy(\"emp_id\") \\\n",
    "  .agg(count(\"*\").alias(\"count\")) \\\n",
    "  .filter(col(\"count\") > 2) \\\n",
    "  .select(\"emp_id\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8dc3c2-bed7-4ad8-81eb-ebc1e4e646ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|    emp_id|      date|   status|\n",
      "+----------+----------+---------+\n",
      "|7175690919|2023-01-01|CANCELLED|\n",
      "|7175690919|2023-12-25|   ACTIVE|\n",
      "|7175690919|2023-12-05|   ACTIVE|\n",
      "|7175690919|2023-12-14|   ACTIVE|\n",
      "|7175690919|2023-12-15|CANCELLED|\n",
      "|7175690919|2023-07-01|   ACTIVE|\n",
      "|7175690919|2023-08-05|   ACTIVE|\n",
      "|7175690919|2023-12-10|   ACTIVE|\n",
      "|7175690919|2023-03-14|   ACTIVE|\n",
      "|7175690919|2023-03-22|   ACTIVE|\n",
      "|7175690919|2023-11-07|   ACTIVE|\n",
      "|7175690919|2023-04-23|   ACTIVE|\n",
      "|7175690919|2023-04-16|   ACTIVE|\n",
      "|7175690919|2023-05-19|   ACTIVE|\n",
      "|7175690919|2023-05-26|   ACTIVE|\n",
      "|7175690919|2023-07-18|   ACTIVE|\n",
      "|7175690919|2024-11-19|   ACTIVE|\n",
      "|7175690919|2024-05-23|   ACTIVE|\n",
      "|7175690919|2024-01-01|   ACTIVE|\n",
      "|7175690919|2024-12-27|   ACTIVE|\n",
      "+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "leave_data = spark.read.option(\"header\", \"true\") \\\n",
    "                   .option(\"inferSchema\", \"true\") \\\n",
    "                   .csv(\"employee_leave_data.csv\")\n",
    "# Step 1: Ensure date is in correct format\n",
    "leave_data = leave_data.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    "leave_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453dd700-4581-4e53-a8d7-01f093024341",
   "metadata": {},
   "source": [
    "For each (emp_id, date):\n",
    "\n",
    "Count how many times each status occurs: \"ACTIVE\" and \"CANCELLED\"\n",
    "\n",
    "Apply logic:\n",
    "\n",
    "If \"CANCELLED\" count > \"ACTIVE\" count → status is \"CANCELLED\"\n",
    "\n",
    "If \"ACTIVE\" count > \"CANCELLED\" count → status is \"ACTIVE\"\n",
    "\n",
    "If counts are equal → status is \"CANCELLED\" (since cancel overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cdb105-d5dc-4ece-a213-6e6feea6efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+---------+------------+\n",
      "|    emp_id|      date|   status|is_active|is_cancelled|\n",
      "+----------+----------+---------+---------+------------+\n",
      "|7175690919|2023-01-01|CANCELLED|        0|           1|\n",
      "|7175690919|2023-12-25|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-12-05|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-12-14|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-12-15|CANCELLED|        0|           1|\n",
      "|7175690919|2023-07-01|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-08-05|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-12-10|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-03-14|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-03-22|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-11-07|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-04-23|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-04-16|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-05-19|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-05-26|   ACTIVE|        1|           0|\n",
      "|7175690919|2023-07-18|   ACTIVE|        1|           0|\n",
      "|7175690919|2024-11-19|   ACTIVE|        1|           0|\n",
      "|7175690919|2024-05-23|   ACTIVE|        1|           0|\n",
      "|7175690919|2024-01-01|   ACTIVE|        1|           0|\n",
      "|7175690919|2024-12-27|   ACTIVE|        1|           0|\n",
      "+----------+----------+---------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Map status to two count columns\n",
    "leave_status_counts = leave_data.withColumn(\"is_active\", when(col(\"status\") == \"ACTIVE\", 1).otherwise(0)) \\\n",
    "                                .withColumn(\"is_cancelled\", when(col(\"status\") == \"CANCELLED\", 1).otherwise(0))\n",
    "leave_status_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59eb94e6-aa8b-4133-87b4-68accda8a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 05:41:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 9:===================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+---------------+\n",
      "|    emp_id|      date|active_count|cancelled_count|\n",
      "+----------+----------+------------+---------------+\n",
      "|7175690919|2023-05-19|           1|              0|\n",
      "| 272449155|2023-11-05|           1|              0|\n",
      "|9154542239|2023-06-08|           1|              0|\n",
      "|9154542239|2023-07-10|           1|              0|\n",
      "|1260589765|2023-05-16|           1|              0|\n",
      "|6727837063|2023-02-12|           1|              0|\n",
      "|9623856554|2024-02-11|           1|              0|\n",
      "|3368105384|2023-05-10|           1|              0|\n",
      "|6619102777|2023-06-18|           1|              0|\n",
      "|6619102777|2023-07-28|           1|              0|\n",
      "|5568316074|2023-02-03|           1|              0|\n",
      "| 706688810|2024-07-20|           1|              0|\n",
      "|4530872653|2024-03-13|           1|              0|\n",
      "|5033623298|2023-09-21|           1|              0|\n",
      "|8022137890|2023-04-13|           1|              0|\n",
      "|5468748102|2024-09-11|           1|              0|\n",
      "|8328042768|2023-07-10|           1|              0|\n",
      "|8523798777|2024-04-23|           1|              0|\n",
      "|3286077338|2024-01-24|           1|              0|\n",
      "| 958054928|2024-10-15|           1|              0|\n",
      "+----------+----------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 3: Group by emp_id + date and sum status flags\n",
    "grouped = leave_status_counts.groupBy(\"emp_id\", \"date\") \\\n",
    "    .agg(\n",
    "        sum(\"is_active\").alias(\"active_count\"),\n",
    "        sum(\"is_cancelled\").alias(\"cancelled_count\")\n",
    "    )\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee44bd8-bf3d-4bd9-b1ad-d5228f9817b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 05:41:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 12:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|    emp_id|      date|status|\n",
      "+----------+----------+------+\n",
      "|7175690919|2023-05-19|ACTIVE|\n",
      "| 272449155|2023-11-05|ACTIVE|\n",
      "|9154542239|2023-06-08|ACTIVE|\n",
      "|9154542239|2023-07-10|ACTIVE|\n",
      "|1260589765|2023-05-16|ACTIVE|\n",
      "|6727837063|2023-02-12|ACTIVE|\n",
      "|9623856554|2024-02-11|ACTIVE|\n",
      "|3368105384|2023-05-10|ACTIVE|\n",
      "|6619102777|2023-06-18|ACTIVE|\n",
      "|6619102777|2023-07-28|ACTIVE|\n",
      "|5568316074|2023-02-03|ACTIVE|\n",
      "| 706688810|2024-07-20|ACTIVE|\n",
      "|4530872653|2024-03-13|ACTIVE|\n",
      "|5033623298|2023-09-21|ACTIVE|\n",
      "|8022137890|2023-04-13|ACTIVE|\n",
      "|5468748102|2024-09-11|ACTIVE|\n",
      "|8328042768|2023-07-10|ACTIVE|\n",
      "|8523798777|2024-04-23|ACTIVE|\n",
      "|3286077338|2024-01-24|ACTIVE|\n",
      "| 958054928|2024-10-15|ACTIVE|\n",
      "+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 4: Apply status logic based on counts\n",
    "leave_data_updated = grouped.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"cancelled_count\") >= col(\"active_count\"), \"CANCELLED\").otherwise(\"ACTIVE\")\n",
    ").select(\"emp_id\", \"date\", \"status\")\n",
    "leave_data_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e119682d-70ae-4de7-af2d-96404d3d816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 05:41:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/05/08 05:41:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "dupes = leave_data_updated.groupBy(\"emp_id\", \"date\").count().filter(\"count > 1\")\n",
    "if dupes.count() > 0:\n",
    "    print(\"⚠️ Duplicate ACTIVE leave records found:\")\n",
    "    dupes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b779ec-7779-444a-9a3b-606e0594ba70",
   "metadata": {},
   "source": [
    "\n",
    "Generate a daily table (at 7:00 UTC) showing currently active employees by designation and count.\n",
    "\n",
    "Active Employees by Designation (Daily @ 7:00 UTC)\n",
    "Input:\n",
    "\n",
    "employee_timeframe_data (already processed and includes status)\n",
    "\n",
    "Logic:\n",
    "\n",
    "Filter records where status = 'Active'\n",
    "\n",
    "Group by designation\n",
    "\n",
    "Count employees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf20c5d-bc5d-4d1c-8fa7-a404fde72846",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#employee_timeline = spark.read.format(\"delta\").load(\"s3://your-bucket/employee_timeframe_delta/\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m active_employees \u001b[38;5;241m=\u001b[39m \u001b[43mdf4\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActive\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesignation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39magg(count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memp_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_emp_count\u001b[39m\u001b[38;5;124m\"\u001b[39m))\\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_emp_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m active_employees\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#active_employees.write.mode(\"overwrite\").format(\"delta\").save(\"s3://your-bucket/reports/active_employee_counts\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df4' is not defined"
     ]
    }
   ],
   "source": [
    "#employee_timeline = spark.read.format(\"delta\").load(\"s3://your-bucket/employee_timeframe_delta/\")\n",
    "\n",
    "active_employees = df4.filter(col(\"status\") == \"Active\") \\\n",
    "    .groupBy(\"designation\") \\\n",
    "    .agg(count(\"emp_id\").alias(\"active_emp_count\"))\\\n",
    "    .sort(\"active_emp_count\")\n",
    "active_employees.show()\n",
    "\n",
    "#active_employees.write.mode(\"overwrite\").format(\"delta\").save(\"s3://your-bucket/reports/active_employee_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994584e-1b6d-43e1-bbe7-04845f868c9e",
   "metadata": {},
   "source": [
    "Potential Leave Abuse > 8% of Working Days (Daily @ 7:00 UTC)\n",
    "Inputs:\n",
    "\n",
    "employee_leave_data.csv\n",
    "\n",
    "employee_leave_calendar_data.csv\n",
    "\n",
    "Generate working days from tomorrow to end of year\n",
    "\n",
    "Exclude:\n",
    "\n",
    "Weekends\n",
    "\n",
    "Public holidays\n",
    "\n",
    "Cancelled leaves\n",
    "\n",
    "Logic:\n",
    "\n",
    "Calculate remaining working days this year.\n",
    "\n",
    "Filter employee_leave_data:\n",
    "\n",
    "status = ACTIVE\n",
    "\n",
    "leave date in future\n",
    "\n",
    "not weekend, not holiday\n",
    "\n",
    "Count leaves per employee\n",
    "\n",
    "If leave_count > 8% of working days, flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c1fac-f753-4066-8e23-cf2414123c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, to_date, dayofweek, explode, sequence, countDistinct, expr, year\n",
    ")\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# ========== Step 1: Setup Dates ==========\n",
    "# today = datetime.utcnow().date()\n",
    "today = datetime(2024,4,25) # when i explicitly gave the date\n",
    "current_date_str = today.strftime('%Y-%m-%d')\n",
    "end_of_year_str = f\"{today.year}-12-31\"\n",
    "run_date_str = today.strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"📅 Running for date range:\", current_date_str, \"to\", end_of_year_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede96c5-b53a-44f5-ba67-516b13058d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 2: Prepare Leave Data ==========\n",
    "leave_data_final = leave_data_updated.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "active_leaves = leave_data_final.filter(\n",
    "    (col(\"status\") == \"ACTIVE\") &\n",
    "    (col(\"date\") >= lit(current_date_str))\n",
    ").dropDuplicates([\"emp_id\", \"date\"])\n",
    "\n",
    "print(\"🟡 Active leave records:\", active_leaves.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67355729-51c2-4980-a587-17ac50c0a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 3: Prepare Holiday Data ==========\n",
    "holidays = calendar_data \\\n",
    "    .withColumn(\"date\", to_date(\"date\")) \\\n",
    "    .filter(col(\"date\") >= lit(current_date_str)) \\\n",
    "    .select(\"date\").distinct() \\\n",
    "    .withColumn(\"holiday\", lit(1))\n",
    "holidays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bb942-6a09-4efe-a0ef-69eec572ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 4: Generate Working Days ==========\n",
    "date_range = spark.createDataFrame([()]).select(\n",
    "    explode(sequence(to_date(lit(current_date_str)), to_date(lit(end_of_year_str)))).alias(\"date\")\n",
    ")\n",
    "\n",
    "working_days = date_range \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"date\")) \\\n",
    "    .filter(~col(\"day_of_week\").isin([1, 7])) \\\n",
    "    .drop(\"day_of_week\") \\\n",
    "    .join(holidays, on=\"date\", how=\"left_anti\") \\\n",
    "    .persist()\n",
    "\n",
    "total_working_days = working_days.count()\n",
    "print(\"📆 Total working days left:\", total_working_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763491ef-6b93-479f-9309-17a564a2b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_working_days == 0:\n",
    "    print(\"⚠️ No valid working days found. Exiting early.\")\n",
    "else:\n",
    "    # ========== Step 5: Join Leaves with Working Days ==========\n",
    "    leaves_on_working_days = active_leaves.join(working_days, on=\"date\", how=\"inner\") \\\n",
    "        .select(\"emp_id\", \"date\").distinct()\n",
    "\n",
    "    print(\"📌 Leaves on working days:\", leaves_on_working_days.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dcfed-abd4-45b1-9a5f-bd74d2c411cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 6: Count Leaves and Flag ==========\n",
    "emp_leave_counts = leaves_on_working_days.groupBy(\"emp_id\") \\\n",
    "                   .agg(countDistinct(\"date\").alias(\"leaves_taken\"))\n",
    "\n",
    "flagged = emp_leave_counts.withColumn(\n",
    "        \"leave_percent\",\n",
    "        (col(\"leaves_taken\") / lit(total_working_days)) * 100\n",
    "    ).withColumn(\n",
    "        \"flagged\", expr(\"CASE WHEN leave_percent > 8 THEN 'Yes' ELSE 'No' END\")\n",
    "    ).withColumn(\"run_date\", lit(run_date_str))\n",
    "\n",
    "# Show result\n",
    "flagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0e856-6caf-471a-9677-4e77c1edb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_flagged = flagged.filter(col(\"flagged\") == \"Yes\").count()\n",
    "print(f\"✅ Employees flagged (leave > 8%): {count_flagged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e64e34-c757-4ff1-afc3-430424cadcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_flagged = flagged.filter(col(\"emp_id\") == '154225493')\n",
    "employee_flagged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130afd5-2138-4ea4-987f-0901f444b33d",
   "metadata": {},
   "source": [
    "Leave Quota Overuse >80% (Monthly on 1st @ 7:00 UTC)\n",
    "Inputs:\n",
    "\n",
    "employee_leave_data.csv\n",
    "\n",
    "employee_leave_quota_data.csv\n",
    "\n",
    "Logic:\n",
    "\n",
    "For current year, count leaves where status = ACTIVE\n",
    "\n",
    "Join with leave quota data\n",
    "\n",
    "If (leave_taken / quota) > 0.8, flag\n",
    "\n",
    "Group by manager (placeholder or you can skip the file-per-manager if no manager field is present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd643669-f431-46c3-b82d-b7c6c8740b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "# ========== Step 1: Setup Reporting Dates ==========\n",
    "# Simulate: Run on the 1st of any month\n",
    "#today = datetime.utcnow().date()\n",
    "today = datetime(2024, 11, 1).date()  # ← change to the 1st of any month you're testing\n",
    "print(today)\n",
    "# Calculate end of previous month as cutoff\n",
    "report_cutoff = today - relativedelta(days=1)  # e.g., 2024-04-30\n",
    "report_cutoff_str = report_cutoff.strftime(\"%Y-%m-%d\")\n",
    "run_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "current_year = today.year\n",
    "\n",
    "print(f\"📊 Reporting for period: Jan 1 to {current_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d73c0-bda7-4cfa-9495-f580fc7770f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 2: Prepare Clean Leave Data ==========\n",
    "leave_data_updated = leave_data_updated.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "# Filter ACTIVE leaves in the current year up to the cutoff\n",
    "valid_leaves = leave_data_updated.filter(\n",
    "    (col(\"status\") == \"ACTIVE\") &\n",
    "    (year(col(\"date\")) == current_year) &\n",
    "    (col(\"date\") <= lit(report_cutoff_str))\n",
    ")\n",
    "valid_leaves.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3c868-4afb-4a2b-969c-15bed5b4d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate date range Jan 1 → report_cutoff\n",
    "date_range = spark.createDataFrame([()]).select(\n",
    "    explode(sequence(\n",
    "        to_date(lit(f\"{current_year}-01-01\")),\n",
    "        to_date(lit(report_cutoff_str))\n",
    "    )).alias(\"date\")\n",
    ")\n",
    "\n",
    "# Remove weekends\n",
    "working_days = date_range.withColumn(\"dow\", dayofweek(\"date\")) \\\n",
    "    .filter(~col(\"dow\").isin([1, 7])) \\\n",
    "    .drop(\"dow\")\n",
    "\n",
    "working_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5f3a-6846-4b03-961f-b058b13af708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join holiday calendar and remove holiday dates\n",
    "holidays = calendar_data.withColumn(\"date\", to_date(\"date\")).select(\"date\").distinct()\n",
    "working_days = working_days.join(holidays, on=\"date\", how=\"left_anti\")\n",
    "print(\"total working days\", working_days.count())\n",
    "\n",
    "# Join with valid leave data\n",
    "leaves_on_working_days = valid_leaves.join(working_days, on=\"date\", how=\"inner\")\n",
    "\n",
    "leaves_on_working_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e823e8b-b026-4122-b9b7-53ef58b968e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count real working leaves\n",
    "leaves_taken = leaves_on_working_days.groupBy(\"emp_id\") \\\n",
    "    .agg(countDistinct(\"date\").alias(\"leaves_taken\"))\n",
    "leaves_taken.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62cbb3-7eae-4a13-9131-3a6da448f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 3: Join with Quota & Flag ==========\n",
    "leave_usage = leaves_taken.join(quota_data, on=\"emp_id\", how=\"inner\") \\\n",
    "    .filter(col(\"year\") == lit(current_year)) \\\n",
    "    .withColumn(\"leave_percent\", (col(\"leaves_taken\") / col(\"leave_quota\")) * 100) \\\n",
    "    .withColumn(\"flagged\", expr(\"CASE WHEN leave_percent > 80 THEN 'Yes' ELSE 'No' END\")) \\\n",
    "    .filter(col(\"flagged\") == \"Yes\") \\\n",
    "    .withColumn(\"run_date\", lit(run_date_str))\n",
    "\n",
    "leave_usage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e68a4-c294-4592-82ed-4d5ec03a47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_leaves = leaves_taken.filter(col(\"emp_id\") == '1226091381')\n",
    "employee_leaves.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cb583-9810-4e14-8a05-e8f966610823",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_count = leave_usage.count()\n",
    "print(f\"🔍 Total flagged employees: {flagged_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fcba2e-5fdc-4649-8d86-f5c4d3d7a211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c1065-7d0a-4c2d-bf25-abd5a353f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
